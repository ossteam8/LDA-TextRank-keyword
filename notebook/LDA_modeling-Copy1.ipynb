{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import platform\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# tokenizer import\n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma\n",
    "\n",
    "#운영체제에 따라 mecab설치 방법이 다름.\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple, Dict\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import kss\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('economy_id.pickle', 'rb') as f:\n",
    "    id_news = pickle.load(f)\n",
    "with open('politic_id.pickle', 'rb') as f:\n",
    "    id_news.extend(pickle.load(f))\n",
    "with open('society_id.pickle', 'rb') as f:\n",
    "    id_news.extend(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('economy_contents.pickle', 'rb') as f:\n",
    "    news = pickle.load(f)\n",
    "with open('politic_contents.pickle', 'rb') as f:\n",
    "    news.extend(pickle.load(f))\n",
    "with open('society_contents.pickle', 'rb') as f:\n",
    "    news.extend(pickle.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heejaesuh/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2238\n"
     ]
    }
   ],
   "source": [
    "def merge_news(news):\n",
    "    lines = \"\"\n",
    "    for i in news:\n",
    "        lines = lines+i\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전에 \"명\" 추가\n",
    "stop = \"이후,부,대,시,바,외,속,차,점,단,후,듯,곳,필요,기여,이용,만,불,원,중,씨,헤럴드,전재,뿐,면,반,관련,기대,제기,우려,번,지적,배포,금지,말,건,분,회,간,내,수,거,게,명,직전,아,휴,아이구,아이쿠,아이고,어,나,우리,저희,따라,의해,을,를,에,의,가,으로,로,에게,뿐이다,의거하여,근거하여,입각하여,기준으로,예하면,예를 들면,예를 들자면,저,소인,소생,저희,지말고,하지마,하지마라,다른,물론,또한,그리고,비길수 없다,해서는 안된다,뿐만 아니라,만이 아니다,만은 아니다,막론하고,관계없이,그치지 않다,그러나,그런데,하지만,든간에,논하지 않다,따지지 않다,설사,비록,더라도,아니면,만 못하다,하는 편이 낫다,불문하고,향하여,향해서,향하다,쪽으로,틈타,이용하여,타다,오르다,제외하고,이 외에,이 밖에,하여야,비로소,한다면 몰라도,외에도,이곳,여기,부터,기점으로,따라서,할 생각이다,하려고하다,이리하여,그리하여,그렇게 함으로써,하지만,일때,할때,앞에서,중에서,보는데서,으로써,로써,까지,해야한다,일것이다,반드시,할줄알다,할수있다,할수있어,임에 틀림없다,한다면,등,등등,제,겨우,단지,다만,할뿐,딩동,댕그,대해서,대하여,대하면,훨씬,얼마나,얼마만큼,얼마큼,남짓,여,얼마간,약간,다소,좀,조금,다수,몇,얼마,지만,하물며,또한,그러나,그렇지만,하지만,이외에도,대해 말하자면,뿐이다,다음에,반대로,반대로 말하자면,이와 반대로,바꾸어서 말하면,바꾸어서 한다면,만약,그렇지않으면,까악,툭,딱,삐걱거리다,보드득,비걱거리다,꽈당,응당,해야한다,에 가서,각,각각,여러분,각종,각자,제각기,하도록하다,와,과,그러므로,그래서,고로,한 까닭에,하기 때문에,거니와,이지만,대하여,관하여,관한,과연,실로,아니나다를가,생각한대로,진짜로,한적이있다,하곤하였다,하,하하,허허,아하,거바,와,오,왜,어째서,무엇때문에,어찌,하겠는가,무슨,어디,어느곳,더군다나,하물며,더욱이는,어느때,언제,야,이봐,어이,여보시오,흐흐,흥,휴,헉헉,헐떡헐떡,영차,여차,어기여차,끙끙,아야,앗,아야,콸콸,졸졸,좍좍,뚝뚝,주룩주룩,솨,우르르,그래도,또,그리고,바꾸어말하면,바꾸어말하자면,혹은,혹시,답다,및,그에 따르는,때가 되어,즉,지든지,설령,가령,하더라도,할지라도,일지라도,지든지,몇,거의,하마터면,인젠,이젠,된바에야,된이상,만큼,어찌됏든,그위에,게다가,점에서 보아,비추어 보아,고려하면,하게될것이다,일것이다,비교적,좀,보다더,비하면,시키다,하게하다,할만하다,의해서,연이서,이어서,잇따라,뒤따라,뒤이어,결국,의지하여,기대여,통하여,자마자,더욱더,불구하고,얼마든지,마음대로,주저하지 않고,곧,즉시,바로,당장,하자마자,밖에 안된다,하면된다,그래,그렇지,요컨대,다시 말하자면,바꿔 말하면,즉,구체적으로,말하자면,시작하여,시초에,이상,허,헉,허걱,바와같이,해도좋다,해도된다,게다가,더구나,하물며,와르르,팍,퍽,펄렁,동안,이래,하고있었다,이었다,에서,로부터,까지,예하면,했어요,해요,함께,같이,더불어,마저,마저도,양자,모두,습니다,가까스로,하려고하다,즈음하여,다른,다른 방면으로,해봐요,습니까,했어요,말할것도 없고,무릎쓰고,개의치않고,하는것만 못하다,하는것이 낫다,매,매번,들,모,어느것,어느,로써,갖고말하자면,어디,어느쪽,어느것,어느해,어느 년도,라 해도,언젠가,어떤것,어느것,저기,저쪽,저것,그때,그럼,그러면,요만한걸,그래,그때,저것만큼,그저,이르기까지,할 줄 안다,할 힘이 있다,너,너희,당신,어찌,설마,차라리,할지언정,할지라도,할망정,할지언정,구토하다,게우다,토하다,메쓰겁다,옆사람,퉤,쳇,의거하여,근거하여,의해,따라,힘입어,그,다음,버금,두번째로,기타,첫번째로,나머지는,그중에서,견지에서,형식으로 쓰여,입장에서,위해서단지,의해되다,하도록시키다,뿐만아니라,반대로,전후,전자,앞의것,잠시,잠깐,하면서,그렇지만,다음에,그러한즉,그런즉,남들,아무거나,어찌하든지,같다,비슷하다,예컨대,이럴정도로,어떻게,만약,만일,위에서 서술한바와같이,인 듯하다,하지 않는다면,만약에,무엇,무슨,어느,어떤,아래윗,조차,한데,그럼에도 불구하고,여전히,심지어,까지도,조차도,하지 않도록,않기 위하여,때,시각,무렵,시간,동안,어때,어떠한,하여금,네,예,우선,누구,누가 알겠는가,아무도,줄은모른다,줄은 몰랏다,하는 김에,겸사겸사,하는바,그런 까닭에,한 이유는,그러니,그러니까,때문에,그,너희,그들,너희들,타인,것,것들,너,위하여,공동으로,동시에,하기 위하여,어찌하여,무엇때문에,붕붕,윙윙,나,우리,엉엉,휘익,윙윙,오호,아하,어쨋든,만 못하다,하기보다는,차라리,하는 편이 낫다,흐흐,놀라다,상대적으로 말하자면,마치,아니라면,쉿,그렇지 않으면,그렇지 않다면,안 그러면,아니었다면,하든지,아니면,이라면,좋아,알았어,하는것도,그만이다,어쩔수 없다,하나,일,일반적으로,일단,한켠으로는,오자마자,이렇게되면,이와같다면,전부,한마디,한항목,근거로,하기에,아울러,하지 않도록,않기 위해서,이르기까지,이 되다,로 인하여,까닭으로,이유만으로,이로 인하여,그래서,이 때문에,그러므로,그런 까닭에,알 수 있다,결론을 낼 수 있다,으로 인하여,있다,어떤것,관계가 있다,관련이 있다,연관되다,어떤것들,에 대해,이리하여,그리하여,여부,하기보다는,하느니,하면 할수록,운운,이러이러하다,하구나,하도다,다시말하면,다음으로,에 있다,에 달려 있다,우리,우리들,오히려,하기는한데,어떻게,어떻해,어찌됏어,어때,어째서,본대로,자,이,이쪽,여기,이것,이번,이렇게말하자면,이런,이러한,이와 같은,요만큼,요만한 것,얼마 안 되는 것,이만큼,이 정도의,이렇게 많은 것,이와 같다,이때,이렇구나,것과 같이,끼익,삐걱,따위,와 같은 사람들,부류의 사람들,왜냐하면,중의하나,오직,오로지,에 한하다,하기만 하면,도착하다,까지 미치다,도달하다,정도에 이르다,할 지경이다,결과에 이르다,관해서는,여러분,하고 있다,한 후,혼자,자기,자기집,자신,우에 종합한것과같이,총적으로 보면,총적으로 말하면,총적으로,대로 하다,으로서,참,그만이다,할 따름이다,쿵,탕탕,쾅쾅,둥둥,봐,봐라,아이야,아니,와아,응,아이,참나,년,월,일,령,영,일,이,삼,사,오,육,륙,칠,팔,구,이천육,이천칠,이천팔,이천구,하나,둘,셋,넷,다섯,여섯,일곱,여덟,아홉,령,영 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.16912007331848\n"
     ]
    }
   ],
   "source": [
    "news_doc = merge_news(news)\n",
    "text_list = kss.split_sentences(news_doc)\n",
    "k = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a2e045c76c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mnouns\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"Noun extractor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/MeCab.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_words = stop.split(',')\n",
    "mecab = Mecab()\n",
    "stime = time.time()\n",
    "for i in news :\n",
    "    sentences = mecab.nouns(i)\n",
    "    result2 = []\n",
    "    for word in sentences :\n",
    "        if word not in stop_words:\n",
    "            result2.append(word)\n",
    "    k.append(result2)\n",
    "print(time.time()-stime)\n",
    "print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(k, min_count = 5, threshold = 0.000000000000000000001)\n",
    "\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "bigram_document = [bigram_model[nouns] for nouns in k]\n",
    "\n",
    "for i in range(len(bigram_document)):\n",
    "    if word in stop_words:\n",
    "        bigram_document.drop(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(bigram_document)\n",
    "corpus = [id2word.doc2bow(doc) for doc in bigram_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_score=[]\n",
    "t_min = 2\n",
    "t_max = 20\n",
    "for i in range(t_min,t_max):\n",
    "    model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=i)\n",
    "    coherence_model = CoherenceModel(model, texts=bigram_document, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model.get_coherence()\n",
    "    print('n=',i,'\\nCoherence Score: ', coherence_lda)\n",
    "    coherence_score.append(coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_sc = np.array(coherence_score)\n",
    "NUM_TOPICS = np.argmax(co_sc)+t_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_term_prob(lda_model):\n",
    "    topic_term_freqs = lda_model.state.get_lambda()\n",
    "    topic_term_prob = topic_term_freqs / topic_term_freqs.sum(axis=1)[:, None]\n",
    "    return topic_term_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus, id2word=id2word, num_topics=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_prob = get_topic_term_prob(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    print(id_news[i],'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [[]]*(NUM_TOPICS+1)#NO Topic0\n",
    "for i in range(NUM_TOPICS):\n",
    "    pp[i] = prepared_data.sorted_terms(topic = i,_lambda=0.6).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index만 추출하기(index가 corpus의 index랑 동일함 ㅇㅇ)\n",
    "idx_topic = [[]]*(NUM_TOPICS+1)\n",
    "for i in range(1,len(pp)-1):\n",
    "    idx_topic[i] = pp[i].iloc[:20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NT_list():\n",
    "    #list size: NUM_TOPICS+1\n",
    "    NT_list = []\n",
    "    for i in range(NUM_TOPICS+1):\n",
    "        NT_list.append([])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#topic별 clustering을 위한 topic_docs 초기화(list[[]]*n 이런 식으로 하면 안됨)\n",
    "topic_docs = build_NT_list()\n",
    "topic_docs_save = build_NT_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append doc_index in the topic_docs cluster\n",
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    topic_list.sort(reverse=True,key = lambda element:element[1])\n",
    "    n = topic_list[0][0] + 1\n",
    "    topic_docs[n].append([i,topic_list[0][1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append doc_index in the topic_docs cluster\n",
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    topic_list.sort(reverse=True,key = lambda element:element[1])\n",
    "    n = topic_list[0][0] + 1\n",
    "    topic_docs_save[n].append([id_news[i],topic_list[0][1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#topic_docs_save[2]\n",
    "#반환해줄 실제 news index와 가중치 저장해둔 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in topic_docs:\n",
    "    i.sort(reverse=True,key = lambda element:element[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cluster = []\n",
    "for i in range(NUM_TOPICS+1):\n",
    "    topic_cluster.append(\"\")\n",
    "for i in range(1,NUM_TOPICS+1):\n",
    "    for j in topic_docs[i]:\n",
    "        topic_cluster[i] = topic_cluster[i]+news[j[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    topic_list.sort(reverse=True,key= lambda element:element[1])\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cluster_sentences = build_NT_list()\n",
    "tcs = build_NT_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,NUM_TOPICS+1):\n",
    "    mecab = Mecab()\n",
    "    stop_words = stop.split(',')\n",
    "    topic_cluster_sentences[i] = kss.split_sentences(topic_cluster[i])\n",
    "    for j in topic_cluster_sentences[i]:\n",
    "        sen_word = []\n",
    "        sentences = mecab.nouns(j)\n",
    "        for word in sentences :\n",
    "            if word not in stop_words:\n",
    "                sen_word.append(word)\n",
    "        tcs[i].append(sen_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_docs = build_NT_list()\n",
    "#construct bigram_docs --> use the model made before\n",
    "for i in range(1,NUM_TOPICS+1):\n",
    "    bigram_docs[i] = [bigram_model[nouns] for nouns in tcs[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_docs = build_NT_list()\n",
    "for i in range(1,NUM_TOPICS+1):    \n",
    "    corpus_docs[i] = [id2word.doc2bow(doc) for doc in bigram_docs[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_doc_ref = build_NT_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "#corpus에서 index만 추출 (index,빈도수) 형태임\n",
    "for i in range(1,NUM_TOPICS+1):\n",
    "    if len(corpus_docs[i]) is not 0:\n",
    "        for j in range(len(corpus_docs[i])):\n",
    "            corp_doc_ref[i].append([])\n",
    "            for k in range(len(corpus_docs[i][j])):\n",
    "                a = corpus_docs[i][j][k][0]\n",
    "                corp_doc_ref[i][j].append(a)\n",
    "\n",
    "print(time.time()-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#키워드 포함된 문장만 추출\n",
    "stime = time.time()\n",
    "corp_doc_topic = build_NT_list()\n",
    "\n",
    "for i in range(1,len(corp_doc_ref)):\n",
    "    for j in range(len(corp_doc_ref[i])):\n",
    "        if len(set(idx_topic[i-1]).intersection(corp_doc_ref[i][j])) is not 0:\n",
    "            corp_doc_topic[i].append(bigram_docs[i][j])\n",
    "\n",
    "print(time.time()-stime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corp_doc_topic[N][M] : N개의 topic, M개의 문장 키워드 포함한 문장만 뽑아낸 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sents(corp_doc_topic = corp_doc_topic, min_count=2, tokenizer=\"mecab\", noun=True):\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x,lowercase=False)\n",
    "    vec = vectorizer.fit_transform(corp_doc_topic)\n",
    "    vocab_idx = vectorizer.vocabulary_\n",
    "    idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}\n",
    "    \n",
    "    return vec, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_matrix(x, min_sim=0.3):\n",
    "    sim_mat = 1 - pairwise_distances(x.T, metric=\"cosine\")\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(\n",
    "    corp_doc_topic = corp_doc_topic,\n",
    "    min_count=2,\n",
    "    min_sim=0.3,\n",
    "    tokenizer=\"mecab\",\n",
    "    noun=True,\n",
    "):\n",
    "\n",
    "    mat, vocab_idx, idx_vocab = vectorize_sents(\n",
    "        corp_doc_topic,min_count=min_count, tokenizer=tokenizer, noun=noun\n",
    "    )\n",
    "\n",
    "    mat = word_similarity_matrix(mat, min_sim=min_sim)\n",
    "\n",
    "    return mat, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(x: np.ndarray, df=0.85, max_iter=50, method=\"iterative\"):\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    A = normalize(mat, axis=0, norm=\"l1\")\n",
    "    N = np.ones(A.shape[0]) / A.shape[0]\n",
    "\n",
    "    if method == \"iterative\":\n",
    "        R = np.ones(A.shape[0])\n",
    "        for _ in range(max_iter):\n",
    "            R = df * np.matmul(A, R) + (1 - df) * N\n",
    "    elif method == \"algebraic\":\n",
    "        R = np.linalg.inv((I - df * A))\n",
    "        R = np.matmul(R, (1 - df) * N)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corp_doc_topic)):\n",
    "    if len(corp_doc_topic[i])!= 0:\n",
    "        mat,vocab_idx, idx_vocab = word_graph(corp_doc_topic[i])\n",
    "        R = pagerank(mat,method =\"iterative\")\n",
    "        topk = 10\n",
    "        idxs = R.argsort()[-topk:]\n",
    "        keywords.append([(idx, R[idx], idx_vocab[idx]) for idx in reversed(idxs)])\n",
    "        #keyword = [(R[idx]) for idx in reversed(idxs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in keywords:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topics():\n",
    "    ext_topic_cluster = dict()\n",
    "    tc = []\n",
    "    for i in range(1,NUM_TOPICS):\n",
    "        top_save = dict()\n",
    "        for j in range(len(topic_docs_save[i])):\n",
    "            top_save[topic_docs_save[i][j][0]] = topic_docs_save[i][j][1]\n",
    "        ext_topic_cluster[i] = [keywords[i-1],top_save]\n",
    "    return ext_topic_cluster, NUM_TOPICS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
